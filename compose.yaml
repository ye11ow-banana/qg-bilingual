services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.cpu
    image: ghcr.io/zakazglobal/qg-bilingual-api:latest-cpu
    env_file: .env
    environment:
      - HF_HOME=/cache/hf
      - QG_SERVER_CONFIG=src/qg_bilingual/server/config.yaml
    volumes:
      - hf-cache:/cache/hf
    ports:
      - "8000:8000"
    deploy:
      resources:
        limits:
          cpus: "2.0"
          memory: "4g"

  demo:
    build:
      context: .
      dockerfile: docker/Dockerfile.cpu
    image: ghcr.io/zakazglobal/qg-bilingual-demo:latest-cpu
    entrypoint: ["bash","docker/entrypoint_demo.sh"]
    environment:
      - DEMO_MODE=http
      - SERVER_URL=http://api:8000
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - HF_HOME=/cache/hf
    volumes:
      - hf-cache:/cache/hf
    ports:
      - "7860:7860"
    depends_on:
      - api

  api-gpu:
    profiles: [gpu]
    build:
      context: .
      dockerfile: docker/Dockerfile.cuda
    image: ghcr.io/zakazglobal/qg-bilingual-api:latest-cuda
    env_file: .env
    environment:
      - HF_HOME=/cache/hf
      - QG_SERVER_CONFIG=src/qg_bilingual/server/config.yaml
    volumes:
      - hf-cache:/cache/hf
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  demo-gpu:
    profiles: [gpu]
    build:
      context: .
      dockerfile: docker/Dockerfile.cuda
    image: ghcr.io/zakazglobal/qg-bilingual-demo:latest-cuda
    entrypoint: ["bash","docker/entrypoint_demo.sh"]
    environment:
      - DEMO_MODE=http
      - SERVER_URL=http://api-gpu:8000
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - HF_HOME=/cache/hf
    volumes:
      - hf-cache:/cache/hf
    ports:
      - "7860:7860"
    depends_on:
      - api-gpu

volumes:
  hf-cache:
